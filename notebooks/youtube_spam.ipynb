{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b5cfb7-f4e0-449a-974f-28777e28c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "import importlib\n",
    "import collections\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "\n",
    "from snorkel.labeling import LabelingFunction, PandasLFApplier, LFAnalysis, filter_unlabeled_dataframe\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from src.data.utils import load_youtube_spam_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212fd2ed-3875-4b78-9b57-11422da42368",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d0dd34-d02b-4907-ad61-8284f65687dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = -1\n",
    "HAM = 0\n",
    "SPAM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58eede70-935c-4ecf-9561-30ad52021055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1586, 175, 195)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_PATH = Path(\"/home/s2210421/dataset/youtube_spam_cmt\")\n",
    "df_train, df_val, df_test = load_youtube_spam_dataset(ROOT_PATH / \"data\", True)\n",
    "Y_test = df_test.label.values\n",
    "\n",
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4baed89f-7ac6-4313-a2bb-90cec824c5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(805, 761)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_spam = df_train[df_train.label == 1]\n",
    "df_train_ham = df_train[df_train.label == 0]\n",
    "\n",
    "len(df_train_spam), len(df_train_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c226d1e-e358-4c8d-adb7-b673757b8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "primitive_examples, primitive_labels = collections.defaultdict(list), collections.defaultdict(list)\n",
    "for i, row in df_train.iterrows():\n",
    "    words = list(set(row[\"text\"].split()))\n",
    "    for w in words:\n",
    "        primitive_examples[w].append(i)\n",
    "        primitive_labels[w].append(row[\"label\"])\n",
    "\n",
    "primitive_probs = {}\n",
    "for w in primitive_labels.keys():\n",
    "    primitive_examples[w] = set(primitive_examples[w])\n",
    "    primitive_probs[w] = sum(primitive_labels[w]) / len(primitive_labels[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae1a88c8-d190-45f5-813a-8d99d5451437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_lookup(x, keywords, label):\n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return -1\n",
    "\n",
    "\n",
    "def make_keyword_lf(keywords, label):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label))\n",
    "\n",
    "\n",
    "def seu(df_train, prim_examples, lfs, prob_labels, pred_labels):\n",
    "    cand_lfs = []\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(df_train.text.tolist())\n",
    "\n",
    "\n",
    "def select_dev_example(df, lfs=[], method=\"random\"):\n",
    "    retry_count = 0\n",
    "    while True:\n",
    "        example = df.sample(n=1).iloc[0]\n",
    "        if len(lfs) and retry_count <= 5:\n",
    "            if method == \"abs\":\n",
    "                has_label = False\n",
    "                for lf in lfs:\n",
    "                    if lf(example) != -1:\n",
    "                        has_label = True\n",
    "                        break\n",
    "                if has_label:\n",
    "                    retry_count += 1\n",
    "                    continue\n",
    "            elif method == \"dis\":\n",
    "                labels = []\n",
    "                for lf in lfs:\n",
    "                    l = lf(example)\n",
    "                    if l != -1:\n",
    "                        labels.append(l)\n",
    "                if len(labels) != 0:\n",
    "                    conf = np.mean(labels)\n",
    "                    if conf < 0.4 or conf > 0.7:\n",
    "                        retry_count += 1\n",
    "                        continue\n",
    "            elif method == \"seu\":\n",
    "                pass\n",
    "\n",
    "        return example\n",
    "\n",
    "\n",
    "def select_primitive(example, selected_prim, prim_probs, prim_thresh, use_weight_probs=False):\n",
    "    words = example.text.lower().split()\n",
    "    cands, cands_w = [], []\n",
    "    for w in words:\n",
    "        if w not in prim_probs or w in selected_prim:\n",
    "            continue\n",
    "\n",
    "        if (example.label == 0 and prim_probs[w] < 1 - prim_thresh) or \\\n",
    "           (example.label == 1 and prim_probs[w] > prim_thresh):\n",
    "            cands.append(w)\n",
    "            if example.label == 1:\n",
    "                cands_w.append(prim_probs[w])\n",
    "            else:\n",
    "                cands_w.append(1 - prim_probs[w])\n",
    "    if len(cands) == 0:\n",
    "        return None\n",
    "    cands_prob = np.exp(cands_w) / np.sum(np.exp(cands_w), axis=0)\n",
    "    if use_weight_probs:\n",
    "        prim = np.random.choice(cands, size=1, p=cands_prob)[0]\n",
    "    else:\n",
    "        prim = np.random.choice(cands, size=1)[0]\n",
    "    return prim\n",
    "\n",
    "\n",
    "def build_primitive_lf(df, lfs, select_method, selected_prim, prim_probs, prim_thres,\n",
    "                       use_weight_probs=False):\n",
    "    prim, example = None, None\n",
    "    while prim is None:\n",
    "        example = select_dev_example(df, lfs, select_method)\n",
    "        prim = select_primitive(example, selected_prim, prim_probs, prim_thres, use_weight_probs)\n",
    "    return make_keyword_lf(keywords=[prim], label=example.label), (prim, example.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d21793",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_trials\": 50,\n",
    "\n",
    "    \"dataset\": \"ytb\",\n",
    "    \"dataset_path\": \"/home/s2210421/dataset/youtube_spam_cmt\",\n",
    "\n",
    "    \"num_iter\": 50,\n",
    "    \"eval_iter_mod\": 5,\n",
    "    \"alternative_draw\": False,\n",
    "    \"select_method\": \"random\",\n",
    "    \"primitive_threshold\": 0.7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b8c4e36-621c-4f5d-8dd1-3a8aa8b1b326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5 - LM: 70.2564 - EM: 62.0513\n",
      "Iter 10 - LM: 74.3590 - EM: 60.0000\n",
      "Iter 15 - LM: 65.1282 - EM: 77.9487\n",
      "Iter 20 - LM: 67.1795 - EM: 81.0256\n",
      "Iter 25 - LM: 67.1795 - EM: 74.3590\n",
      "Iter 30 - LM: 67.1795 - EM: 66.6667\n",
      "Iter 35 - LM: 65.1282 - EM: 63.0769\n",
      "Iter 40 - LM: 68.2051 - EM: 64.6154\n",
      "Iter 45 - LM: 68.2051 - EM: 65.6410\n",
      "Iter 50 - LM: 68.7179 - EM: 64.6154\n"
     ]
    }
   ],
   "source": [
    "lfs, selected_prim = [], {}\n",
    "use_weight_probs = False\n",
    "\n",
    "segment_df = None\n",
    "segment_flag = False\n",
    "\n",
    "for iter in range(1, config[\"num_iter\"] + 1):\n",
    "    if iter == 1:\n",
    "        for _ in range(3):\n",
    "            if config[\"alternative_draw\"]:\n",
    "                segment_df = df_train_spam if segment_flag else df_train_ham\n",
    "                segment_flag = not segment_flag\n",
    "            else:\n",
    "                segment_df = df_train\n",
    "            lf, prim = build_primitive_lf(segment_df, lfs, config[\"select_method\"], selected_prim, primitive_probs,\n",
    "                                          config[\"primitive_threshold\"], use_weight_probs)\n",
    "            lfs.append(lf)\n",
    "            selected_prim[prim[0]] = prim[1]\n",
    "    else:\n",
    "        if config[\"alternative_draw\"]:\n",
    "            segment_df = df_train_spam if segment_flag else df_train_ham\n",
    "            segment_flag = not segment_flag\n",
    "        else:\n",
    "            segment_df = df_train\n",
    "        lf, prim = build_primitive_lf(segment_df, lfs, config[\"select_method\"], selected_prim, primitive_probs,\n",
    "                                      config[\"primitive_threshold\"], use_weight_probs)\n",
    "        lfs.append(lf)\n",
    "        selected_prim[prim[0]] = prim[1]\n",
    "\n",
    "    if iter % config[\"eval_iter_mod\"] == 0:\n",
    "        applier = PandasLFApplier(lfs=lfs)\n",
    "        L_train = applier.apply(df=df_train, progress_bar=False)\n",
    "        L_test = applier.apply(df=df_test, progress_bar=False)\n",
    "\n",
    "        label_model = LabelModel(cardinality=2, verbose=False)\n",
    "        label_model.fit(L_train=L_train, n_epochs=500, progress_bar=False)\n",
    "        lm_acc = label_model.score(\n",
    "            L=L_test, Y=df_test.label.values, tie_break_policy=\"random\")[\"accuracy\"] * 100\n",
    "\n",
    "        prob_labels = label_model.predict_proba(L=L_train)\n",
    "        df_train_filtered, prob_labels_filtered = filter_unlabeled_dataframe(\n",
    "            X=df_train, y=prob_labels, L=L_train)\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "        X_test = vectorizer.transform(df_test.text.tolist())\n",
    "\n",
    "        lm_labels = probs_to_preds(probs=prob_labels_filtered)\n",
    "        end_model = LogisticRegression(C=1e3, solver=\"liblinear\")\n",
    "        end_model.fit(X=X_train, y=lm_labels)\n",
    "        em_acc = end_model.score(X=X_test, y=df_test.label.values) * 100\n",
    "\n",
    "        lf_analysis = LFAnalysis(L=L_train, lfs=lfs)\n",
    "\n",
    "        print(f\"Iter {iter} - LM: {lm_acc:.4f} - EM: {em_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a48e716-9bf3-4d21-b10f-c749271ab949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(df_train.text.tolist())\n",
    "end_model = LogisticRegression(C=1e3, solver=\"liblinear\")\n",
    "end_model.fit(X=X_train, y=probs_to_preds(prob_labels))\n",
    "pred_labels = probs_to_preds(end_model.predict_proba(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73c2fc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_list = [0, 3, 4, 6]\n",
    "\n",
    "lm_uncer = -np.sum(prob_labels * np.log(prob_labels), axis=-1)\n",
    "\n",
    "corr = pred_labels[ex_list] == np.array([1] * len(ex_list)) \n",
    "\n",
    "_w = np.where(corr == 1, corr, -1)\n",
    "util_score = np.sum(lm_uncer[ex_list] * _w)\n",
    "\n",
    "np.sum(corr) / corr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb704f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author    marie smith               \n",
       "date      2015-05-25T22:01:29.580000\n",
       "text      music                     \n",
       "label     0                         \n",
       "video     3                         \n",
       "Name: 86, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df_train.iloc[101]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7bf826d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[101].equals(df_train.iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba982953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
